---
title: "When companies make products for users instead of profit"
tags: ["ai", "dark forest internet"]
date: 2024-07-30T22:50:51+02:00
ingress: "The internet has gotten a lot worse with the release of genAI, which seems to be widely developed and used in spite of major problems. Where do we go from here?"
---

A few days ago, I caved and got myself a [Kagi](https://kagi.com) subscription. TL;DR: Kagi is a paid search engine, and I got a recommendation about it around 8 months ago. Yet, it took until now for me to act on it. The reason? SearchGPT, and the assumption that both Google and Microsoft are working on a competitor to shove into their respective search engines, with no regard for search quality. 

Kagi, admittedly, does have an AI push as well. But unlike both Microsoft and Google, it isn't shoved down the users' throats. It exists, and some of the automatic functionality can be outright disabled, but it doesn't influence normal search results.

However, the point of mentioning Kagi is that I saw something I haven't seen in a while. Especially in a search engine. Kagi is a search engine made by people who understand what users want from a search engine. The results feel like Google results before Google managed to get worse than Bing (and before Bing declined just has hard and managed to get worse than itself, which is impressive, but not in a good way), but better. There's so many QOL features. I'm only 4 days in, so I haven't had the chance to explore everything, but lenses, first-class keyboard shortcuts, URL rewriting, and changing the ranking of sites by preference are a few of the things I've already come to love. 

These are quality of life features you get when you have a product made by a company that understands the product, the target audience, and isn't limited by shareholders who don't understand the domain. This used to be a higher priority for many companies. Maybe not as specific features as this, but value for end-users was on the list. Now, particularly in the post-genAI era, this has been thrown out the window by most major companies.

## Chatbots - chatbots everywhere

Following the release of ChatGPT, and it subsequently exploding in popularity, do you remember what companies did? I remember it painfully well. 

Everyone wanted their own chatbot. Search engines got chatbots. Websites got chatbots. Social media platforms got chatbots. Documentation sites got chatbots. This has kept going in the nearly two years since OpenAI released GPT onto the internet, in one of the single most irresponsible and dumb actions they've done to date. 

Most of this, however, is an annoyance. But there are exceptions. One of the more recent examples is a non-profit hotline that fired its staff and replaced them with a chatbot, and got a surprised pikachu moment when [it started giving out bad advice](https://www.forbes.com/sites/chriswestfall/2023/05/31/non-profit-helpline-fires-staff-shifts-to-chatbot-solution/).

There's so many ways AI contributes to a net negative, and I cannot cover them all. But it isn't without reason that:

> Generative AI is the nuclear bomb of the Information Age.  
> \- [Kyle Hill](https://www.youtube.com/watch?v=PaVjQFMg7L0)

Aside the many consequences to information sharing, genAI has now become a hype bubble that shareholders, investors, and companies use as an excuse to push these [anti-environmental](https://www.technologyreview.com/2023/12/05/1084417/ais-carbon-footprint-is-bigger-than-you-think/) misinformation generators[^2][^3][^4] instead of focusing on their users.

There's a fine line between managing the profitability and UX tradeoff, but many companies now seem to just not care about either - pushing AI now inflates the valuation of their companies, which is fantastic for shareholders and potential investors. But not so much for end-users, or society at large.

### Obligatory note: When AI actually has value

Both here and elsewhere, I present a generally negative opinion of AI. However, this primarily applies to generative AI, and _especially_ when it's used in uncontrolled settings, or on large amounts of data where classical algorithms currently give better results. At least if they were tweaked to deal with sites abusing SEO. AIs have a margin of error, so any result an AI gives needs to be checked by humans. A lot of genAI output (based on my observations) is not checked before it's used. 

Obligatory I'm not a lawyer, but this kind of use of ChatGPT's also violates OpenAI's terms of service. Their ToS [requires output be checked](https://openai.com/policies/sharing-publication-policy/), but from the use I've seen, more often than not, this is disregarded. There's also no consequences from this, because it's an incredibly difficult requirement to enforce. But then they turn around and [violate YouTube's ToS](https://fortune.com/2024/04/04/openai-youtube-clear-violation-terms-service-ai-sora-training/), which just nails in how little they actually care about their products being used ethically. 

OpenAI being evil aside, there are cases where (non-generative) AI provides net societal value. While a common example, there are experimental medical AIs that are able to catch cancer that humans miss. AI can solve many problems that humans and classical algorithms can't, though their claims need to be manually validated due to inaccuracies and errors.

Generative AI in particular is a bad solution in search of a problem. Just like NFTs. Just like crypto. Just like blockchain. 

## How Stack Exchange, Inc. sacrificed its users for AI company money 

I've written a lot about this lately, so I'm going to be short here. 

In July 2024, Stack Exchange, Inc. announced that they were [once again going to try to restrict the data dump](https://meta.stackexchange.com/questions/401324/announcing-a-change-to-the-data-dump-process?cb=1), by moving it off archive.org to their own infrastructure, and (arguably) making legal threats to anyone trying to archive the data. 

This means that if Stack Exchange, Inc. ever goes under or axes the public platform, Stack Overflow data would just be gone[^1]. This is Stack Exchange, Inc.'s own attempt to have a [Reddit moment](https://www.wheresyoured.at/are-we-watching-the-internet-die/) - squeezing money out of AI companies, with the community not even making it as a footnote.

Stack Exchange is a :sparkles: special :sparkles: case, however. You see, Stack Exchange, Inc. defends this bullshit decision by claiming they want to encourage "socially responsible AI". If  you have an hour to spare (or even if you don't - it's a damn good video), [Philosophy Tube](https://www.youtube.com/watch?v=AaU6tI2pb3M) has a video explaining what ethical/responsible/sustainable/other buzzword AI actually means far more generally and in far more detailed than I will here.

Relevant here is that Stack Exchange, Inc., while outwardly claiming otherwise, does not care about its users, [its community](https://meta.stackexchange.com/a/401503/332043), nor the sustainability of the community. It's a cashgrab taking money from AI companies so their investors and CEO can get a payday - to add insult to injury, under a year after laying off [28% of their staff](https://meta.stackexchange.com/q/393806/332043).

## Where Stack Exchange, Inc. (and many other companies) went wrong

In June 2023, moderators and users in the Stack Exchange network declared [a moderation strike](https://meta.stackexchange.com/q/389811/332043). It took a couple months to finish up the negotiations, after which, far from everyone returned.

I have a lot of things I could say about Stack Exchange, Inc.'s atrocious handling of both the strike and the data dump, but that is not the point of this post. There was a revision 1 of the data dump restrictions that involved a request review by lawyers. Over a couple weeks, mods managed to talk them down, before they abruptly posted revision 3 on a Friday. At this point, the revision had only been out internally for a day.

How is any of this relevant, you ask? **They failed to understand their users**. They failed to understand their product, its purpose and use, and they failed to understand the significance of  the data dump to its users. The community never crossed their mind, aside for a brief moment when they realised they needed an excuse to try to reduce resistance from the community, and other interested parties.

They only care about the part where they want to charge AI companies for access to the data (i.e. profit), blatantly disregarding how AI companies have most likely **already scraped all the data**, and blatantly disregarding how other non-AI entities, notably its users, have an interest in getting access to the data they helped make.

They designed for the third parties they wanted to profit from, not for the users that lay the foundation for everything else. 

This is what most other companies are doing right now. They're not creating products for the end-users, they're creating products for investors and shareholders, to inflate their own value, regardless of the cost for end-users in their target audience.

## Where do we go from here?

The truth is, I have no idea. Forces far greater than any individual, and more powerful than entire countries are making these moves pay off. The world, right now, runs on a system where infinite growth and short-term benefits are valued far more than long-term sustainability and profitability.

The exceptions to the rule, like Kagi, are few and far between. The non-corporate solutions being developed right now often get run by underfunded and overworked open-source developers. There are still a good few long-standing exceptions here as well. Linux also prioritises users, but due to a lack of resources, they lack the capacity to go through with user-oriented features in a reasonable period of time. 

While not a particularly optimistic ending, it seems more likely that the [dark forest internet](https://www.youtube.com/watch?v=JrcbH0ge2WE) is going to take hold, and that things are going to keep getting worse. The long-term, right now, is unpredictable. Barring drastic actions from end-users, this is going to be a situation that's difficult to get out of. This is a problem we both need to fight, and a problem that's incredibly difficult to fight.

Maybe we'll see a rise in possibly non-profits working to add value for users rather than investors in the long term, or an uptick in FOSS products meant to compete with big tech, and an influx of investments in that area. But right now, big tech and the shareholders supporting it are forcing us into the dark forest internet - and it's too early to see what, if anything, is on the other side.

[^1]: This is a worst-case where no one tries to create an archive by other means. I'm already working on [a data dump downloader](https://github.com/LunarWatcher/se-data-dump-transformer/) for this new anti-community format, but even if this wasn't possible, archivists have gone further to archive content before.
[^2]: https://arxiv.org/abs/2405.13554
[^3]: https://arxiv.org/abs/2108.09293
[^4]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11128619/
